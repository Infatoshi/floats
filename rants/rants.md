# Floating Point Arithmetic (fp4, fp8, fp16, bf16, fp16, fp32... and integers too!)
Something that has deeply bothered me and has caused a lot of pain in learning about low-level optimizations and how systems work is when it comes to the actual atoms that you work with themselves. The numbers that are multiplying in massive matrices, those integers or those floating-point numbers that aren't just ones and zeros, but it's more than that. Integers are very easy to understand, but floating-point, I think it's important that we understand that better and understand why some formats might actually be okay, whereas they classically may have not been okay, and to understand where the bottlenecks might be in hardware for these specific compute units. Like, for instance, floating-point units inside of a GPU will take up significantly more silicon space than an integer unit. Now, let's dive in. So to start off, you've got, in the following diagram here, you've got the floating-point 32 format, which we're all probably used to, and we also have the integer 32 format. Now, let's compare these and just understand the full spectrum of what each of these will cover, and let's lay out some common terms. So first, in the integer diagram, we've got a bunch of ones and zeros. There are 32 of them in a line. And all this is, is 2 to the power of, and then whatever that number is in that specific slot. So on the slot, on the, and we're going to go, the left is the highest power, and the right is the lowest power. So if we start all the way on the right and go up, the right will be 2 to the power of 0. That's the first slot. And then the second one will be 2 to the power of 1. And then third will be 2 to the power of 2, and et cetera, all the way up to 2 to the power of 31, which is the final last. It's the 32nd slot. We start at 0. It goes from 0 to 31. Now, you can, you will only get a positive number with this, because there's no way to signal really a negative here, if we just go all the way up to 2 to the power of 31. Now, on the other hand, if we were to, say, discard that 2 to the power of 31, and instead of put a little sign bit there, the sign bit is the sign of the number. So negative or positive. And so you end up, instead of having this long list that stretches really far and only is positive, you have this symmetrical line here on the whole number line, where that's the range of values you can have. So you can go from negative 2 to the power of 30 to positive 2 to the power of 30. That's your range now. And you can only represent this with whole numbers. So there's no decimal numbers here. Again, this is integers. Now, in the other diagram, we notice that things are in a bit of a weird format, and we need to put them in this format in order to A, get range, B, get precision, and C to represent decimals themselves. Now, this might be unintuitive at first, but we've got the same thing. In floating point numbers, you will always have the sign bit as the first one, very far left. And then you will have the mantis bits, which, in the case of floating point 32, I can't actually remember what it is, but you have the exponent bits, and then you have the mantis bits after. Now, the exponent bits basically say, this is how much we're going to exponentiate it. This is going to be the large range of this number. This is how much we're going to scale it. And then the mantis bits are those which actually represent the precision of the number. Now, it's not exactly... This is what I was misconceived with at first, is instead of having a... Instead of literally going 2 to the power of 0, 2 to the power of 1, 2 to the power of 2, like an integer, and then having the decimals just be like 2 to the power of negative 1, 2 to the power of negative 2, 2 to the power of negative 3, Instead of going down that route, we do some interesting stuff in the actual hardware, which I'm not going to go over in this particular article, but we basically use some really interesting tricks that will be more intuitive in a second once we walk through some examples on how exactly this will work. And I want to lay out the rules specifically on how this works with examples so that this builds the best intuition in your head.

Now one option would be to go just ask an LLM and see what answer it gives you and go and walk through that on your own, which I would absolutely encourage. But at the same time, you clicked on this article, I'm going to get you your click-worthy from this. So let's take a look at the diagram here. This is another thing that language models will not give you is intuitive diagrams like these that are sort of easy to follow along and with lots of nice labels everywhere. Anyways, let's continue. 


# seperate rant
Now, floating point format is really, really cool, and I'm going to kind of show you visually why it makes sense to use low precision floating point numbers as opposed to integers or even higher precision floating point numbers. Very low precision. I think FP8 and even FP4 is great options. Now, in FP8 you typically have a sine bit, four exponent bits, and three mantisa bits, or one sine bit, five exponent bits, and two mantisa bits. Basically the exponent says, you know, the exponent is literally exponent, like you can go negative, you can say have like, all the way from like two to the power of negative seven all the way up to two to the power of positive eight or positive seven or whatever. And point is, that is like a, that is like a range that you get, right? That's a range. And then you've got the mantisa bits, which are going to indicate basically the precision of those. So the more mantisa bits you have, the more specific you can be in terms of decimal numbers. But it doesn't really matter at the end of the day, because when it comes to being specific, um, the, when you go all the way to like the lowest possible difference in floating point numbers, there is still, it is still extremely close. And a very, uh, the difference between like when we're doing, uh, when we're doing back propagation and we have to update a weight and say layer norm, and it's like one E negative five. It's a very small number, but we can still go down to that precision. And then we need to be specific about how much we're changing it. Like, oh, like one E negative five, as opposed to one point five each and negative five. There's like a very small difference there, but we can actually be very specific about this in floating point precision. And then the only time where this wouldn't be as precise is when we, you know, have a higher exponent bit, because you're basically taking that mantisa that is not very precise. And then you scale it up to some massive number. And now the difference, once you get into like the maybe hundreds range, where you're not no longer decimal numbers, you're now playing around with like fairly large integers. The range there, the step between each of those actually is going to increase, but that's okay. Because when we need such high ranges, um, when we have those search, such large steps, that typically means that we don't actually care about precision that much. because if you get an attention score or some activation that fires off as like 300, then it doesn't matter if it's like 300 point five or 300. The point is, is that it's 300 and we have that specific. Um, we have a very good idea of how big that range is. And even if it goes up like 10, it doesn't matter that much. The fact that it's like 300, we know that that's the, the floating point eight format supports that range. And that's really what we care about here. So this is going to make more sense as we sort of walk through with hand examples and visualize what exactly this looks like. And here I actually wrote a Python script to help us understand, um, and visualize with mad plot lib, what that range looks like for FPA numbers and help us understand why, um, the new hopper generation GPUs. Well, they're not super new anymore, but at the time they were new and why FPA format was, um, like basically overpowered for doing inference. Um, also training workloads. It's quite good with as well. And the deep seek V three technical report that deep seek V three is the base model that, uh, that allowed deep seek R one to be, uh, trained with, or to be post trained into a reasoning model. Now that base was actually, uh, run in FPA precision. It was trained in FPA. Um, there are, there are 16 bit version of those weights, but it's a mainly a, um, FPA training pipeline. And that is one of the coolest things ever, I think. 